## ğŸ—ƒï¸âœ¨ Mebox - ğŸ™Œ Open Source Knowledge Base For AI Agents

[![US](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/us.png "Canada") English](/readme/en.md) -
[![Spain](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/es.png "Spain") EspaÃ±ol](/readme/es.md) -
[![France](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/fr.png "France") FranÃ§ais](/readme/fr.md) -
[![Germany](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/de.png "Germany") Deutschland](/readme/de.md) -
[![China](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/cn.png "China") ä¸­å›½](/readme/cn.md) -
[![India](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/in.png "China") à¤¹à¤¿à¤‚à¤¦à¥€](/readme/in.md) -
[![Korea](https://raw.githubusercontent.com/stevenrskelton/flag-icon/master/png/16/country-4x3/kr.png "Korea") í•œêµ­ì–´](/readme/kr.md)

<img src="https://github.com/christivn/mebox/blob/main/img/mebox.jpg?raw=true">

Mebox is an open-source alternative to OpenAI's `file_search` tool, designed to efficiently process, store, and retrieve file-based information using Supabase and embeddings.

### ğŸ’¡ Why Use Mebox?

ğŸ’° **Open-source & Cost-effective** - Open source alternative.  
âš¡ **Fast & Scalable** - Uses Supabase Vector for high-performance retrieval.  
ğŸ” **Accurate Search** - Supports semantic search with embeddings.  
ğŸ“‚ **Multiple file formats:** `pdf`, `json`, `csv`, `xlsx`, `txt`, `md`, `xml`, `yaml`, `ini`, `log`, `bat`, `py`, `js`, `java`, `cpp`, `html`  
ğŸŒ **Multilingual support:** +12 languages supported  
ğŸ”¹ **Multiple AI:** +300 models  

### Index

- [ğŸ“– About Mebox](#-about-mebox)  
- [ğŸ”§ Use Cases](#-use-cases)
- [ğŸ‘€ WebUI](#-webui)
- [âš¡ API](#-api)
- [ğŸ› ï¸ Installation & Setup](#%EF%B8%8F-installation--setup)  
- [ğŸ“ Documentation](#-documentation)
  - [ğŸ“„ Transform File to Text](#-transform-file-to-text)  
  - [ğŸ§© Chunks Generation](#-chunks-generation)  
  - [ğŸ”— Embedding the Chunks](#-embedding-the-chunks)  
  - [ğŸ’¾ Database](#-database) 
  - [ğŸ” Search & Retrieval](#-search--retrieval)  
- [ğŸ¤ Contributing](#-contributing)  
- [ğŸ“œ License](#-license)  

<br>

## ğŸ”§ Use Cases

Mebox can be used in various applications that require efficient file-based information retrieval and knowledge management:

ğŸ“š **AI-Powered Knowledge Bots** â€“ Create AI assistants that can answer questions based on custom document databases.  
ğŸ¯ **Recommendation Algorithms** â€“ Build personalized recommendation systems using semantic search.  
ğŸ” **Legal & Compliance Research** â€“ Quickly retrieve relevant documents and case laws based on contextual search.  
ğŸ“š **Academic & Research Tools** â€“ Enable efficient searching through large collections of research papers and books.  
ğŸ’¼ **Enterprise Knowledge Management** â€“ Organize and search internal company documentation with ease.  
ğŸ¤– **Chatbot Integration** â€“ Enhance chatbots with document-based memory for more informed responses.  
ğŸ“‚ **Automated Data Extraction** â€“ Process and analyze structured and unstructured text data from various file formats.  
ğŸ’¬ **Predefined Response Bots** â€“ Develop bots that provide predefined responses based on stored information.  
ğŸ“ˆ **Market Analysis & Trend Detection** â€“ Analyze large datasets for market trends and insights.  
ğŸ¢ **Customer Support Automation** â€“ Streamline customer support by enabling automated query resolution.  
**And more...**

<br>

## ğŸ‘€ WebUI
`Coming soon`

<br>

## âš¡ API
`Coming soon`

- Create agent
- Remove agent
- Make a query to agent
- Get list of uploaded agent filenames
- Upload file into a agent

**Stream API**

`Coming soon`

<br>

## ğŸ› ï¸ Installation & Setup

**Requisitos:**

- Api Key de Openrouter
- Supabase con `pgvector`

```bash
# Clone the repository
git clone https://github.com/christivn/mebox.git

# Install dependencies
pip install -r requirements.txt

# Set up environment variables (Supabase keys, embedding model API keys, etc.)
export SUPABASE_URL=your_supabase_url
export SUPABASE_KEY=your_supabase_key

# Run the application
python main.py
```

<br><br>

---

<br><br>

# ğŸ“ Documentation
Este proyecto abarca todo el proceso de gestiÃ³n y bÃºsqueda de embeddings a travÃ©s de varias etapas. Comienza con la carga de archivos en el sistema, que luego se transforman a texto para extraer la informaciÃ³n relevante. El texto se divide en *chunks* (fragmentos) para su posterior procesamiento, y cada *chunk* es convertido en un **embedding** utilizando un modelo de aprendizaje automÃ¡tico. Estos embeddings son almacenados eficientemente en **Supabase**, donde se indexan utilizando **HNSW** para optimizar las bÃºsquedas de similitud. Finalmente, el sistema permite realizar bÃºsquedas rÃ¡pidas y precisas mediante la **similitud de coseno**, facilitando la recuperaciÃ³n de informaciÃ³n relevante a partir de grandes volÃºmenes de datos.

<br>

## ğŸ“„ Transform File to Text
Extracts meaningful text from the uploaded file using different parsing methods based on file type.

**getTextFromWebpage:** `website URL`

**getTextFromTXT:** `txt`, `md`, `xml`, `yaml`, `ini`, `log`, `bat`, `py`, `js`, `java`, `cpp`, `html`, `json`

**getTextFromPDF:** `pdf`

**getTextFromImage:** `jpg`, `jpge`, `png`

**getTextFromCSV:** `csv`, `xlsx`

**getTextFromMicrosoftOffice:** `doc`, `docx`, `pptx`

**getTextFromEmail:** `eml`, `eml/zip`

<br>

## ğŸ§© Chunks Generation
En el procesamiento de texto para modelos de lenguaje y recuperaciÃ³n de informaciÃ³n, es comÃºn dividir documentos largos en fragmentos mÃ¡s pequeÃ±os denominados chunks. Esta segmentaciÃ³n facilita el anÃ¡lisis, mejora la precisiÃ³n en la recuperaciÃ³n de informaciÃ³n y permite manejar mejor las limitaciones de longitud.

La siguiente imagen muestra un ejemplo de segmentaciÃ³n de texto en chunks con distintos tamaÃ±os y solapamiento. Cada chunk estÃ¡ resaltado con colores diferentes para visualizar su separaciÃ³n dentro del documento original. Este mÃ©todo es clave para optimizar tareas de bÃºsqueda semÃ¡ntica y generaciÃ³n de texto basada en contexto.

**Mebox Default Settings:**
- ğŸ“ **Chunk size:** 512 tokens
- ğŸ”¢ **Max chunks in context:** 20

<img src="https://github.com/christivn/mebox/blob/main/img/text_splitter.png?raw=true" width="600px">

<br>

## ğŸ”— Embedding the Chunks
Each chunk is embedded using a powerful embedding model, allowing for efficient similarity-based search.

**Default Settings:**
- ğŸ§  **Embedding model:** `thenlper/gte-small`
- ğŸ“ **Dimensions:** 384

The GTE models are trained by **Alibaba DAMO Academy**. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. 

| Model Name | Model Size (GB) | Dimension | Sequence Length | Average (56) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10) | Summarization (1) | Classification (12) |
|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| [**gte-small**](https://huggingface.co/thenlper/gte-small) | **0.07** | **384** | **512** | **61.36** | **44.89** | **83.54** | **57.7** | **49.46** | **82.07** | **30.42** | **72.31** |

**Comparison with other embedding models:**
| Model Name | Model Size (GB) | Dimension | Sequence Length | Average (56) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10) | Summarization (1) | Classification (12) |
|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| e5-large-v2 | 1.34 | 1024| 512 | 62.25 | 44.49 | 86.03 | 56.61 | 50.56 | 82.05 | 30.19 | 75.24 |
| e5-base-v2 | 0.44 | 768 | 512 | 61.5 | 43.80 | 85.73 | 55.91 | 50.29 | 81.05 | 30.28 | 73.84 |
| text-embedding-ada-002 | - | 1536 | 8192 | 60.99 | 45.9 | 84.89 | 56.32 | 49.25 | 80.97 | 30.8 | 70.93 |
| e5-small-v2 | 0.13 | 384 | 512 | 59.93 | 39.92 | 84.67 | 54.32 | 49.04 | 80.39 | 31.16 | 72.94 |

**Example `gte-small`embedding:**

```
tensor([[-4.8426e-01, -3.5470e-01,  2.5043e-01,  8.7683e-02,  1.0782e-01,
         -5.7016e-03, -8.1829e-02,  4.4422e-01, -9.2456e-03, -3.5152e-01,
          1.5013e-02, -4.2973e-01,  2.2862e-01,  4.3781e-01,  4.5499e-01,
          2.3649e-02,  1.0961e-01,  1.0125e-01, -4.1820e-01,  3.3270e-01,
          7.0393e-01, -9.8001e-03,  1.0716e-01, -4.6404e-01,  1.4286e-01,
         -9.9045e-03, -1.2405e-01, -1.5022e-01, -3.2292e-01, -1.2242e+00,
          ... more
          3.3178e-03, -7.0972e-01,  1.1175e-01,  6.8678e-01, -2.0580e-02,
          8.3930e-02, -5.8170e-01,  5.7685e-02,  3.8488e-01, -9.4733e-02,
         -2.3249e-01, -1.0366e-01,  1.0068e-01,  2.1887e-01,  1.5462e-01,
         -2.4715e-01, -3.2600e-01, -5.0547e-01, -5.0383e-01, -1.7109e-01,
         -2.3582e-01,  1.5043e-01,  7.6588e-02,  5.1266e-01]],
       grad_fn=<MeanBackward1>)
```

<br>

## ğŸ’¾ Database

En este proyecto, utilizamos Supabase como base de datos para almacenar y gestionar los embeddings generados por un modelo de aprendizaje automÃ¡tico. Para facilitar la bÃºsqueda eficiente de datos similares, implementamos un Ã­ndice basado en el algoritmo HNSW (Hierarchical Navigable Small World) y utilizamos la similitud de coseno para medir la cercanÃ­a entre los vectores de embeddings.

**Â¿Por quÃ© HNSW?**

HNSW (Hierarchical Navigable Small World) es un algoritmo de bÃºsqueda eficiente de vecinos mÃ¡s cercanos en espacios de alta dimensiÃ³n. Se utiliza para mejorar el tiempo de respuesta en la bÃºsqueda de los embeddings mÃ¡s cercanos en grandes volÃºmenes de datos. En lugar de realizar una bÃºsqueda exhaustiva a travÃ©s de todos los vectores, HNSW organiza los datos en una estructura jerÃ¡rquica, lo que permite realizar bÃºsquedas rÃ¡pidas con un nÃºmero reducido de comparaciones.

<img src="https://github.com/christivn/mebox/blob/main/img/HNSW.png?raw=true" width="450px">

**Ventajas de HNSW:**
- **BÃºsqueda eficiente:** Reduce significativamente el tiempo de consulta en grandes volÃºmenes de datos.  
- **Escalabilidad:** Es adecuado para trabajar con grandes bases de datos de embeddings sin comprometer mucho el rendimiento.  
- **PrecisiÃ³n:** Ofrece resultados de alta calidad en la bÃºsqueda de vecinos mÃ¡s cercanos, lo que es fundamental cuando se busca similitud entre embeddings.  

<br>

**Similitud de Coseno**

La similitud de coseno es una medida utilizada para calcular la similitud entre dos vectores en un espacio de alta dimensiÃ³n. Se define como el coseno del Ã¡ngulo entre dos vectores, lo cual indica quÃ© tan similares son en tÃ©rminos de direcciÃ³n, independientemente de su magnitud.

La fÃ³rmula de la similitud de coseno es:

<img src="https://github.com/christivn/mebox/blob/main/img/cosine-distance.png?raw=true" width="450px">
Donde:

AA y BB son los vectores de los embeddings que estamos comparando.  
âˆ¥Aâˆ¥ y âˆ¥Bâˆ¥ son las normas (o longitudes) de los vectores.  

Un valor de similitud de coseno cercano a 1 indica que los vectores son muy similares, mientras que un valor cercano a 0 indica que los vectores son muy diferentes.

**RepresentaciÃ³n 3D simplificada (Similitud de Coseno de 2 embbedings):**

<img src="https://github.com/christivn/mebox/blob/main/img/cosine-similarity.png?raw=true" width="450px">

<br>

## ğŸ” Search & Retrieval

En los sistemas de recuperaciÃ³n aumentada de generaciÃ³n (Retrieval-Augmented Generation, RAG), es fundamental mejorar la calidad de la informaciÃ³n recuperada para optimizar la generaciÃ³n de respuestas. Una estrategia clave es la segmentaciÃ³n de documentos en chunks y la posterior bÃºsqueda basada en embeddings semÃ¡nticos.

La siguiente imagen ilustra el proceso de recuperaciÃ³n de chunks relevantes a partir de una consulta embebida, incorporando chunks vecinos para enriquecer el contexto. Este enfoque permite reducir la fragmentaciÃ³n del conocimiento y mejorar la coherencia de las respuestas generadas.

<img src="https://github.com/christivn/mebox/blob/main/img/chunks-strategies.jpg?raw=true" width="550px">

1. **RepresentaciÃ³n de la consulta como embedding**  
   Se genera un vector de embedding a partir de la consulta del usuario utilizando un modelo de representaciÃ³n semÃ¡ntica (por ejemplo, un modelo basado en *transformers* como SBERT o OpenAI embeddings). Este embedding se emplea para realizar una bÃºsqueda en una base de datos de *chunks* previamente indexados.  

2. **BÃºsqueda en el Ã­ndice de *chunks***  
   El Ã­ndice contiene mÃºltiples *chunks* de documentos, cada uno con su embedding asociado. Se calcula la similitud entre el embedding de la consulta y los embeddings de los *chunks* en la base de datos (usualmente mediante *cosine similarity* o *dot product*). Los *chunks* mÃ¡s relevantes se seleccionan como candidatos.  

3. **IncorporaciÃ³n de contexto con *chunks* vecinos**  
   Para mitigar problemas de pÃ©rdida de contexto y mejorar la coherencia de la informaciÃ³n recuperada, se incluyen *chunks* vecinos adyacentes a los mÃ¡s relevantes. Esto permite que el modelo tenga acceso a informaciÃ³n contextual adicional.  

4. **GeneraciÃ³n de *chunks* enriquecidos**  
   Los *chunks* seleccionados y sus vecinos se combinan para formar *context-enriched chunks*, los cuales son utilizados en etapas posteriores, como *prompting* en un modelo generativo (*e.g.*, GPT) o como entrada en un sistema de respuesta a preguntas.  

Este enfoque mejora la precisiÃ³n en la recuperaciÃ³n de informaciÃ³n al proporcionar contexto adicional, reduciendo la fragmentaciÃ³n del conocimiento y optimizando la calidad de las respuestas generadas.

- **K-NN:** 2 (Default)
- **Neighboring Chunks Pairs:** 1 (Default)

**CSV & XLSX:**
- **K-NN:** 20 (Default)
- **Neighboring Chunks Pairs:** 0 (Default)


<br><br>

---

<br>

## ğŸ¤ Contributing

Welcome contributions! Feel free to open issues, submit pull requests, or suggest improvements.

ğŸ“© **Contact:** Reach out via GitHub Issues or Discussions.

<br>

## ğŸ“œ License

MIT License Copyright (c) 2025 Christian Ramos. See [LICENSE](https://github.com/christivn/mebox/blob/main/LICENSE) for details.
